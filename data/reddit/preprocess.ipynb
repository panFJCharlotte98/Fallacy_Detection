{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dev': {1: 316, 2: 25, 3: 1},\n",
      " 'test': {1: 152, 2: 18, 7: 1},\n",
      " 'train': {1: 1108, 2: 76, 3: 8, 4: 2, 5: 1}}\n",
      "{'dev': {'Ad Populum': 39,\n",
      "         'Appeal to Authority': 43,\n",
      "         'Appeal to Nature': 42,\n",
      "         'Appeal to Tradition': 42,\n",
      "         'Appeal to Worse Problems': 48,\n",
      "         'False Dilemma': 42,\n",
      "         'Hasty Generalization': 40,\n",
      "         'Slippery Slope': 46},\n",
      " 'test': {'Ad Populum': 20,\n",
      "          'Appeal to Authority': 21,\n",
      "          'Appeal to Nature': 21,\n",
      "          'Appeal to Tradition': 21,\n",
      "          'Appeal to Worse Problems': 24,\n",
      "          'False Dilemma': 21,\n",
      "          'Hasty Generalization': 20,\n",
      "          'Slippery Slope': 23},\n",
      " 'train': {'Ad Populum': 137,\n",
      "           'Appeal to Authority': 148,\n",
      "           'Appeal to Nature': 145,\n",
      "           'Appeal to Tradition': 147,\n",
      "           'Appeal to Worse Problems': 167,\n",
      "           'False Dilemma': 148,\n",
      "           'Hasty Generalization': 144,\n",
      "           'Slippery Slope': 159}}\n",
      "train: 1195\n",
      "dev + test: 513\n",
      "dev: 342\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pprint\n",
    "import ast\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer    \n",
    "\n",
    "def preprocess_reddit(splits, root):\n",
    "    label_map = {\n",
    "        'authority': 'Appeal to Authority',\n",
    "        'blackwhite': \"False Dilemma\",\n",
    "        'hasty_generalization': \"Hasty Generalization\",\n",
    "        'natural': 'Appeal to Nature',\n",
    "        'population': 'Ad Populum',\n",
    "        'slippery_slope': 'Slippery Slope',\n",
    "        'tradition': 'Appeal to Tradition',\n",
    "        'worse_problems': 'Appeal to Worse Problems'\n",
    "    }\n",
    "    all_data = {}\n",
    "    fal_value_count = {}\n",
    "    n_fal_span_count = {}\n",
    "    label_count = {}\n",
    "    for sp in splits:\n",
    "        id = 0\n",
    "        all_data[sp] = []\n",
    "        n_fal_span_count[sp] = []\n",
    "        label_count[sp] = []\n",
    "        file = open(os.path.join(root, f\"{sp}.tsv\"), 'r') \n",
    "        # a = file.readline() \n",
    "        # # The first line consist of headings of the record, so we will store it in an array and move to next line in input_file. \n",
    "        # titles = [t.strip() for t in a.split('\\t')] \n",
    "        for line in file:\n",
    "            id += 1\n",
    "            items = [t.strip() for t in line.split('\\t')]\n",
    "            tokens = ast.literal_eval(items[1])\n",
    "            tk_class = ast.literal_eval(items[4])\n",
    "            text_str = \" \".join(TreebankWordDetokenizer().detokenize(tokens).encode('ascii', errors='ignore').strip().decode('ascii').split())\n",
    "            fal_spans = []\n",
    "            fal_span_labels = []\n",
    "            fal_span = []\n",
    "            for i, (tk, tk_label) in enumerate(zip(tokens, tk_class)):\n",
    "                if i < (len(tk_class) - 1):\n",
    "                    if (tk_label != \"none\"):\n",
    "                        if (tk_class[i+1] == tk_label):\n",
    "                            fal_span.append(tk)\n",
    "                        else:\n",
    "                            fal_span.append(tk)\n",
    "                            fal_spans.append(fal_span)\n",
    "                            fal_span_labels.append(tk_label)\n",
    "                            fal_span = []\n",
    "                else:\n",
    "                    if tk_label != \"none\":\n",
    "                        fal_span.append(tk)\n",
    "                        fal_spans.append(fal_span)\n",
    "                        fal_span_labels.append(tk_label)\n",
    "            \n",
    "            if fal_spans:            \n",
    "                n_fal_span_count[sp].append(len(fal_spans))\n",
    "                if len(set(fal_span_labels)) > 1:\n",
    "                    print(set(fal_span_labels))\n",
    "                # if len(fal_spans) > 1:\n",
    "                #     print(fal_spans)\n",
    "                span_strs = []\n",
    "                for s_ls in fal_spans:\n",
    "                    span_str = \" \".join(TreebankWordDetokenizer().detokenize(s_ls).strip(\" ....\").encode('ascii', errors='ignore').strip().decode('ascii').split())\n",
    "                    try:\n",
    "                        assert span_str in text_str\n",
    "                    except:\n",
    "                        print(span_str)\n",
    "                        print(text_str)\n",
    "                        print()\n",
    "                    text_str = text_str.replace(span_str, \"<\" + span_str + \">\")\n",
    "                    span_strs.append(span_str)\n",
    "                #text_str = \" \".join(text_str.split())\n",
    "                if len(span_strs) == 1:\n",
    "                    all_fal_span_str = span_strs[0]\n",
    "                else:\n",
    "                    all_fal_span_str = \"; \".join([f'''{str(i+1)}. \"{fal}\"''' for i, fal in enumerate(span_strs)])\n",
    "                #all_fal_span_str = \" \".join(all_fal_span_str.split())\n",
    "                assert len(set(fal_span_labels)) == 1\n",
    "                label = list(set(fal_span_labels))[0]\n",
    "                assert items[6] != \"\" and items[6] is not None\n",
    "                if items[5] != \"None\":\n",
    "                    text = f\"Topic: {items[6]}\\nComment: {items[5]}\\nComment Reply: {text_str}\"\n",
    "                else:\n",
    "                    text = f\"Topic: {items[6]}\\nComment: {text_str}\"\n",
    "                one_data = {\n",
    "                    'id': id,\n",
    "                    'text': text,\n",
    "                    'fal_text': all_fal_span_str,\n",
    "                    'label': [label_map[label]]\n",
    "                }\n",
    "                \n",
    "                all_data[sp].append(one_data)\n",
    "                label_count[sp].append(label_map[label])\n",
    "    n_span_count = {}\n",
    "    for sp, count_ls in n_fal_span_count.items():\n",
    "        n_span_count[sp] = {}\n",
    "        for c in count_ls:\n",
    "            if c not in n_span_count[sp]:\n",
    "                n_span_count[sp][c] = 1\n",
    "            else:\n",
    "                n_span_count[sp][c] += 1\n",
    "    print(pprint.pformat(n_span_count))\n",
    "\n",
    "    n_label_count = {}\n",
    "    for sp, lb_ls in label_count.items():\n",
    "        n_label_count[sp] = {}\n",
    "        for l in lb_ls:\n",
    "            if l not in n_label_count[sp]:\n",
    "                n_label_count[sp][l] = 1\n",
    "            else:\n",
    "                n_label_count[sp][l] += 1\n",
    "    print(pprint.pformat(n_label_count))\n",
    "    \n",
    "    json.dump(all_data['train'], open(f'train.json', 'w'), indent=4)\n",
    "    json.dump(all_data['dev'] + all_data['test'], open(f'test.json', 'w'), indent=4)\n",
    "    json.dump(all_data['dev'], open(f'dev.json', 'w'), indent=4)\n",
    "\n",
    "    print(f\"train: {len(all_data['train'])}\")\n",
    "    print(f\"dev + test: {len(all_data['dev'] + all_data['test'])}\")\n",
    "    print(f\"dev: {len(all_data['dev'])}\")\n",
    "    return\n",
    "splits = ['train', 'test', 'dev']\n",
    "preprocess_reddit(splits, root='./raw')\n",
    "\n",
    "#第一个输出的是一个data examples中fallacy spans的数量：例子的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open('./dev_copy.tsv', 'r')\n",
    "# a = file.readline() \n",
    "# # The first line consist of headings of the record, so we will store it in an array and move to next line in input_file. \n",
    "# first_line = [t.strip() for t in a.split('\\t')]\n",
    "# first_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5\n",
      "2\n",
      "4\n",
      "3\n",
      "3\n",
      "4\n",
      "2\n",
      "5\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3,4,5]\n",
    "b =[5, 4, 3, 2,1]\n",
    "for i, (m,n) in enumerate(zip(a,b)):\n",
    "    print(m)\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Getting 12k is 1.5-2h of grinding. You screwed up. Nobody decided that training TV Def was a good idea but you. I don't see any reason why there should be any leeway here. What's next? People being allowed to refight a luma because they accidentally killed it / ran from it? Re-picking your starter because you decided you made the wrong decision? Reverse a breeding process because people forgot to put a strain in? Why not just implement a safe feature if you cater to the carelessness of people\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "t_list = ast.literal_eval(first_line[1])\n",
    "print(type(t_list))\n",
    "TreebankWordDetokenizer().detokenize(t_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"Getting\", \"12k\", \"is\", \"1.5-2h\", \"of\", \"grinding.\", \"You\", \"screwed\", \"up.\", \"Nobody\", \"decided\", \"that\", \"training\", \"TV\", \"Def\", \"was\", \"a\", \"good\", \"idea\", \"but\", \"you.\", \"I\", \"do\", \"n\\\\\\'t\", \"see\", \"any\", \"reason\", \"why\", \"there\", \"should\", \"be\", \"any\", \"leeway\", \"here.\", \"What\", \"\\\\\\'s\", \"next\", \"?\", \"People\", \"being\", \"allowed\", \"to\", \"refight\", \"a\", \"luma\", \"because\", \"they\", \"accidentally\", \"killed\", \"it\", \"/\", \"ran\", \"from\", \"it\", \"?\", \"Re-picking\", \"your\", \"starter\", \"because\", \"you\", \"decided\", \"you\", \"made\", \"the\", \"wrong\", \"decision\", \"?\", \"Reverse\", \"a\", \"breeding\", \"process\", \"because\", \"people\", \"forgot\", \"to\", \"put\", \"a\", \"strain\", \"in\", \"?\", \"Why\", \"not\", \"just\", \"implement\", \"a\", \"safe\", \"feature\", \"if\", \"you\", \"cater\", \"to\", \"the\", \"carelessness\", \"of\", \"people\"]'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_line[1].replace(\"\\\"n\\'t\\\"\", \"'n\\\\'t'\").replace(\"\\\"\\'s\\\"\", \"'\\\\'s'\").replace(\"', '\", \"\\\", \\\"\").replace(\"['\", \"[\\\"\").replace(\"']\", \"\\\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'fjz78w3',\n",
       " 'ori_coi': \"Getting 12k is 1.5-2h of grinding.  You screwed up. Nobody decided that training TV Def was a good idea but you.   I don't see any reason why there should be any leeway here.   What's next? People being allowed to refight a luma because they accidentally killed it / ran from it?    Re-picking your starter because you decided you made the wrong decision?   Reverse a breeding process because people forgot to put a strain in?   Why not just implement a safe feature if you cater to the carelessness of people\",\n",
       " 'multi': ['none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'slippery_slope',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none',\n",
       "  'none'],\n",
       " 'binary_annotations': ['non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy',\n",
       "  'non_fallacy'],\n",
       " 'tokenized_coi': ['Getting',\n",
       "  '12k',\n",
       "  'is',\n",
       "  '1.5-2h',\n",
       "  'of',\n",
       "  'grinding.',\n",
       "  'You',\n",
       "  'screwed',\n",
       "  'up.',\n",
       "  'Nobody',\n",
       "  'decided',\n",
       "  'that',\n",
       "  'training',\n",
       "  'TV',\n",
       "  'Def',\n",
       "  'was',\n",
       "  'a',\n",
       "  'good',\n",
       "  'idea',\n",
       "  'but',\n",
       "  'you.',\n",
       "  'I',\n",
       "  'do',\n",
       "  \"n't\",\n",
       "  'see',\n",
       "  'any',\n",
       "  'reason',\n",
       "  'why',\n",
       "  'there',\n",
       "  'should',\n",
       "  'be',\n",
       "  'any',\n",
       "  'leeway',\n",
       "  'here.',\n",
       "  'What',\n",
       "  \"'s\",\n",
       "  'next',\n",
       "  '?',\n",
       "  'People',\n",
       "  'being',\n",
       "  'allowed',\n",
       "  'to',\n",
       "  'refight',\n",
       "  'a',\n",
       "  'luma',\n",
       "  'because',\n",
       "  'they',\n",
       "  'accidentally',\n",
       "  'killed',\n",
       "  'it',\n",
       "  '/',\n",
       "  'ran',\n",
       "  'from',\n",
       "  'it',\n",
       "  '?',\n",
       "  'Re-picking',\n",
       "  'your',\n",
       "  'starter',\n",
       "  'because',\n",
       "  'you',\n",
       "  'decided',\n",
       "  'you',\n",
       "  'made',\n",
       "  'the',\n",
       "  'wrong',\n",
       "  'decision',\n",
       "  '?',\n",
       "  'Reverse',\n",
       "  'a',\n",
       "  'breeding',\n",
       "  'process',\n",
       "  'because',\n",
       "  'people',\n",
       "  'forgot',\n",
       "  'to',\n",
       "  'put',\n",
       "  'a',\n",
       "  'strain',\n",
       "  'in',\n",
       "  '?',\n",
       "  'Why',\n",
       "  'not',\n",
       "  'just',\n",
       "  'implement',\n",
       "  'a',\n",
       "  'safe',\n",
       "  'feature',\n",
       "  'if',\n",
       "  'you',\n",
       "  'cater',\n",
       "  'to',\n",
       "  'the',\n",
       "  'carelessness',\n",
       "  'of',\n",
       "  'people'],\n",
       " 'parent': 'None',\n",
       " 'title': 'The weakener fruits costs need to be adjusted'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_dict = {\n",
    "    \"id\": first_line[0],\n",
    "    \"ori_coi\": first_line[2],\n",
    "    \"multi\": ast.literal_eval(first_line[4]),\n",
    "    \"binary_annotations\": ast.literal_eval(first_line[3]),\n",
    "    \"tokenized_coi\": ast.literal_eval(first_line[1]),\n",
    "    \"parent\": first_line[5],\n",
    "    \"title\" : first_line[6]\n",
    "}\n",
    "one_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"id\": \"fjz78w3\", \n",
      "\"tokenized_coi\": ['Getting', '12k', 'is', '1.5-2h', 'of', 'grinding.', 'You', 'screwed', 'up.', 'Nobody', 'decided', 'that', 'training', 'TV', 'Def', 'was', 'a', 'good', 'idea', 'but', 'you.', 'I', 'do', 'n\\'t', 'see', 'any', 'reason', 'why', 'there', 'should', 'be', 'any', 'leeway', 'here.', 'What', '\\'s', 'next', '?', 'People', 'being', 'allowed', 'to', 'refight', 'a', 'luma', 'because', 'they', 'accidentally', 'killed', 'it', '/', 'ran', 'from', 'it', '?', 'Re-picking', 'your', 'starter', 'because', 'you', 'decided', 'you', 'made', 'the', 'wrong', 'decision', '?', 'Reverse', 'a', 'breeding', 'process', 'because', 'people', 'forgot', 'to', 'put', 'a', 'strain', 'in', '?', 'Why', 'not', 'just', 'implement', 'a', 'safe', 'feature', 'if', 'you', 'cater', 'to', 'the', 'carelessness', 'of', 'people'], \n",
      "\"ori_coi\": \"Getting 12k is 1.5-2h of grinding. You screwed up. Nobody decided that training TV Def was a good idea but you. I don\\'t see any reason why there should be any leeway here. What\\'s next? People being allowed to refight a luma because they accidentally killed it  ran from it? Re-picking your starter because you decided you made the wrong decision? Reverse a breeding process because people forgot to put a strain in? Why not just implement a safe feature if you cater to the carelessness of people\", \n",
      "\"binary_annotations\": ['non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy', 'non_fallacy'], \n",
      "\"multiclass_annotations\": ['none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'slippery_slope', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none', 'none'], \n",
      "\"parent\": \"None\", \n",
      "\"title\": \"The weakener fruits costs need to be adjusted\"\n",
      "}\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 3 column 19 (char 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(json_str)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#print(pprint.pformat(json_str))\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/software/anaconda3/envs/phd/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/software/anaconda3/envs/phd/lib/python3.11/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/software/anaconda3/envs/phd/lib/python3.11/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 3 column 19 (char 38)"
     ]
    }
   ],
   "source": [
    "#.replace(\"\\'\", \"\\\"\")\n",
    "#.replace(\"\\'\", \"\\\"\")\n",
    "#.replace(\"\\'\", \"\\\"\")\n",
    "json_str = r\"\"\"{{\n",
    "\"id\": \"{id}\", \n",
    "\"tokenized_coi\": {tokenized_coi}, \n",
    "\"ori_coi\": \"{ori_coi}\", \n",
    "\"binary_annotations\": {binary}, \n",
    "\"multiclass_annotations\": {multi}, \n",
    "\"parent\": \"{parent}\", \n",
    "\"title\": \"{title}\"\n",
    "}}\"\"\".format(\n",
    "id=first_line[0],\n",
    "tokenized_coi=first_line[1].replace(\"\\\"n\\'t\\\"\", \"'n\\\\'t'\").replace(\"\\\"\\'s\\\"\", \"'\\\\'s'\").replace(\"', '\", \"\\\", \\\"\").replace(\"['\", \"[\\\"\").replace(\"']\", \"\\\"]\"),\n",
    "ori_coi=\" \".join(first_line[2].split()).replace(\"'\", \"\\\\'\").replace(\"/\", \"\"),\n",
    "binary=first_line[3],\n",
    "multi=first_line[4],\n",
    "parent=first_line[5],\n",
    "title=first_line[6]\n",
    ")\n",
    "print(json_str)\n",
    "#print(pprint.pformat(json_str))\n",
    "json.loads(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"id\": 11, \"title\": \"Kate Steinle\\'s death at the hands of a Mexican national became a flashpoint in the immigration debate  here\\'s the story behind her killing\", \"pre_text\": [\"San Francisco, meanwhile, has adjusted its policy to notify ICE if they are releasing suspected undocumented immigrants who face charges of serious or violent felonies.\", \"\\\\\"This tragedy could have been prevented if San Francisco had simply turned the alien over to ICE as we requested, instead of releasing him back onto the streets,\\\\\" ICE Director Thomas Homan said in a statement on Thursday.\"], \"fal_span\": \"politicians across this country continue to endanger the lives of Americans with sanctuary policies while ignoring the harm inflicted on their constituents.\\\\\"\", \"text\": \"\\\\\"It is unconscionable that <politicians across this country continue to endanger the lives of Americans with sanctuary policies while ignoring the harm inflicted on their constituents.\\\\\">\", \"post_text\": [\"But ICE has faced criticism of its own over not seeking a judicial warrant to legally obtain custody of Garcia Zarate when it discovered he had been transferred into San Francisco\\'s custody.\", \"The agency has argued that obtaining judicial warrants are unnecessary and would place too much burden on ICE officials and federal courts.\"], \"label\": [\"Whataboutism\"]}'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = json.load(open('./dev.json'))\n",
    "data[1]\n",
    "json.dumps(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"text\": \"I\\'d like to help you. \\\\\"You are bad.\\\\\" She\\'s my friend. I\\'dont mind.\\\\\"\", \"list\": [\"a\", \"b\", \"c\", \"she\\'s cute\"]}'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case = {\"text\": \"I'd like to help you. \\\"You are bad.\\\" She's my friend. I'dont mind.\\\"\", \"list\": ['a', 'b', 'c', \"she's cute\"]}\n",
    "json.dumps(case)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
