{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69e05eb3-24da-4c82-b301-5c37c63074f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 1267\n",
      "train: 1222\n",
      "{'Ad Hominem': 171, 'Slippery Slope': 44, 'Appeal to Emotion': 777, 'Appeal to False Authority': 180, 'False Causality (Post Hoc Fallacy)': 50}\n",
      "test: 154\n",
      "test: 150\n",
      "{'Ad Hominem': 21, 'Appeal to Emotion': 96, 'Appeal to False Authority': 22, 'Slippery Slope': 5, 'False Causality (Post Hoc Fallacy)': 6}\n",
      "dev: 136\n",
      "dev: 132\n",
      "{'False Causality (Post Hoc Fallacy)': 5, 'Appeal to Emotion': 86, 'Appeal to False Authority': 18, 'Slippery Slope': 4, 'Ad Hominem': 19}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "import nltk.data\n",
    "import json\n",
    "import regex\n",
    "# Reader of CONLL file\n",
    "def preprocess_elecdebate(conll_paths):\n",
    "    os.makedirs('./new_data', exist_ok=True)\n",
    "    label_dict = {\n",
    "        'AdHominem': 'Ad Hominem', \n",
    "        'AppealtoEmotion': 'Appeal to Emotion', \n",
    "        'AppealtoAuthority': 'Appeal to False Authority', \n",
    "        'Slogans': 'Slogans', \n",
    "        'Slipperyslope': 'Slippery Slope', \n",
    "        'FalseCause': 'False Causality (Post Hoc Fallacy)'\n",
    "    }\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    for conll_path in conll_paths:\n",
    "        split = conll_path.split(\".\")[0]\n",
    "        sentences = []\n",
    "        with open(conll_path, \"r\") as f:\n",
    "            words, marked, labels = \"\", \"\", []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    #print(words)\n",
    "                    #print(labels)\n",
    "                    assert len(labels) == 1\n",
    "                    fallacy = {\n",
    "                        \"id\": len(sentences),\n",
    "                        \"pre-text\": [],\n",
    "                        \"text\": [],\n",
    "                        \"post-text\": [],\n",
    "                        \"label\": labels\n",
    "                    }\n",
    "                    words = \" \".join(regex.sub(r\"\\s+-\\s+\", \"-\", words.encode('ascii', errors='ignore').strip().decode('ascii')).split())\n",
    "                    marked = \" \".join(regex.sub(r\"\\s+-\\s+\", \"-\", marked.encode('ascii', errors='ignore').strip().decode('ascii')).split())\n",
    "                    sents = tokenizer.tokenize(words)\n",
    "                    for sent in sents:\n",
    "                        if sent not in marked:\n",
    "                            fallacy[\"text\"].append(sent)\n",
    "                        else:\n",
    "                            start = marked.find(sent)\n",
    "                            if start == 0 or marked[start-1] != '|':\n",
    "                                if len(fallacy[\"text\"]) > 0:\n",
    "                                    fallacy[\"post-text\"].append(sent)\n",
    "                                else:\n",
    "                                    fallacy[\"pre-text\"].append(sent)  \n",
    "                            else:\n",
    "                                fallacy[\"text\"].append(sent)\n",
    "                    fallacy['text'] = '<' + \" \".join(fallacy['text']) + '>'\n",
    "                    fallacy['pre-text'] = \" \".join(fallacy['pre-text'])\n",
    "                    fallacy['post-text'] = \" \".join(fallacy['post-text'])\n",
    "                    sentences.append(fallacy)\n",
    "                    words, marked, labels = \"\", \"\", []\n",
    "                else:\n",
    "                    splits = line.replace(\"\\u2019\", \"'\").split(\"\\t\")\n",
    "                    words += splits[1] + \" \"\n",
    "                    if splits[-1][0] in ['B', 'I']:\n",
    "                        marked += '|||'\n",
    "                    marked += splits[1] + \" \"\n",
    "    \n",
    "                    if (splits[-1].split('-')[0] == 'B') and (len(labels) == 0 or label_dict[splits[-1].split('-')[1]] != labels[-1]):\n",
    "                        labels.append(label_dict[splits[-1].split('-')[1]])\n",
    "            f.close()\n",
    "        \n",
    "        print(f\"{split}: {len(sentences)}\")\n",
    "        # json.dump(sentences, open(f'{split}.json', 'w'),indent=4)\n",
    "\n",
    "        new_data = []\n",
    "        fal_value_count = {}\n",
    "        for s in sentences:\n",
    "            assert len(s['label']) == 1\n",
    "            lb = s['label'][0]\n",
    "            if lb != 'Slogans':\n",
    "                s['id'] = len(new_data) + 1\n",
    "                new_data.append(s)\n",
    "                if lb not in fal_value_count:\n",
    "                    fal_value_count[lb] = 1\n",
    "                else:\n",
    "                    fal_value_count[lb] += 1\n",
    "        print(f\"{split}: {len(new_data)}\")\n",
    "        json.dump(new_data, open(f'./new_data/{split}.json', 'w'),indent=4)\n",
    "        print(fal_value_count)\n",
    "            \n",
    "    return\n",
    "raw_files = ['train.conll', 'test.conll', 'dev.conll']\n",
    "preprocess_elecdebate(raw_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcb8566c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ad Hominem\n",
      "Appeal to Emotion\n",
      "Appeal to Authority\n",
      "Slogans\n",
      "Slippery Slope\n",
      "False Cause\n"
     ]
    }
   ],
   "source": [
    "label_dict = {\n",
    "        'AdHominem': 'Ad Hominem', \n",
    "        'AppealtoEmotion': 'Appeal to Emotion', \n",
    "        'AppealtoAuthority': 'Appeal to Authority', \n",
    "        'Slogans': 'Slogans', \n",
    "        'Slipperyslope': 'Slippery Slope', \n",
    "        'FalseCause': 'False Cause'\n",
    "    }\n",
    "for k, v in label_dict.items():\n",
    "    print(v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
